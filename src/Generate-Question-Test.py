"""Test and Evaluate Seq2Seq Language Model."""

import argparse
import json
import logging
import os
import typing

import evaluate  # type: ignore
import nltk  # type: ignore
import numpy as np
import torch
from transformers import (  # type: ignore
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    BloomForCausalLM,
    BloomTokenizerFast,
    EvalPrediction,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    T5TokenizerFast,
)

from src.Load_Data import load_datasets
from src.util import (
    compute_bert_score,
    compute_blue,
    compute_meteor,
    compute_rouge,
    get_wandb_tags,
    init_args,
    read_config_file,
)

logging.basicConfig(
    format="%(asctime)s %(levelname)-4s [%(name)s:%(lineno)d] - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)
metrics: typing.Dict[str, bool] = {}


def get_latest_checkpoint_path(config: dict) -> str:
    """Get last checkpoint to test on most recently trained model.

    Args:
        config: config file for training model
    """
    model_zoo = list(os.listdir(config["output_dir"]))
    model_checkpoint_models = []
    for m in model_zoo:
        if m.startswith(config["model_checkpoint"].split("/")[-1]):
            model_checkpoint_models.append(m)
    model_checkpoint_models.sort()
    model = "models/" + model_checkpoint_models[0]
    checkpoints = os.listdir(model)
    latest_checkpoint = "checkpoint-0"

    for checkpoint in checkpoints:
        if checkpoint.startswith("checkpoint-"):
            if int(latest_checkpoint.split("-")[-1]) < int(
                checkpoint.split("-")[-1]
            ):
                latest_checkpoint = checkpoint

    model_checkpoint = os.path.join(model, latest_checkpoint)

    if os.path.exists(model_checkpoint):
        logger.info(f"Using directory {model_checkpoint} for testing")
    else:
        logger.error(
            FileNotFoundError(
                f"The directory {model_checkpoint}\
                    does not exist. There are no greater\
                    checkpoints past this checkpoint"
            ),
            exc_info=True,
        )
    return model_checkpoint


def predict(
    to_predict: str, model: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer
) -> str:
    """Generates a response from the loaded model.

    Args:
        to_predict: Fed into the model as input.
        model: The Seq2Seq model.
        tokenizer: Used to tokenize input for the Seq2Seq model.

    Returns:
        the response generated by the requested Seq2Seq model.
    """
    input_ids = tokenizer(to_predict, return_tensors="pt").to(device)
    outputs = model.generate(input_ids["input_ids"])
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def compute_metrics(eval_pred: EvalPrediction) -> typing.Dict[str, float]:
    """Computes a custom metric combining BLEU and ROUGE.

    Using both bleu and rouge scores, compute_metrics evaluates the
    model's predictions comparing it to the target. 4 kinds
    of rouge scores and 1 bleu score is returned.

    Args:
        eval_pred: Evaluation results from the test dataset.
        rouge: If rouge should be used as a metric.
        bleu: If bleu should be used as a metric.

    Returns:
        Rouge and Bleu scores if requested.
    """
    global metrics

    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt")

    rouge = metrics["rouge"]
    bleu = metrics["bleu"]

    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(
        predictions, skip_special_tokens=True
    )
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Rouge expects a newline after each sentence
    decoded_preds = [
        "\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds
    ]
    decoded_labels = [
        "\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels
    ]

    # Note that other metrics may not have a `use_aggregator` parameter
    # and thus will return a list, computing a metric for each sentence.
    prediction_lens = [
        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions
    ]
    bleu_rouge_score = {}

    compute_rouge(
        decoded_preds, decoded_labels, prediction_lens, bleu_rouge_score
    )

    compute_blue(
        decoded_preds, decoded_labels, prediction_lens, bleu_rouge_score
    )

    compute_bert_score(decoded_preds, decoded_labels, bleu_rouge_score)

    compute_meteor(decoded_preds, decoded_labels, bleu_rouge_score)
    return bleu_rouge_score


def preprocess_data(
    data: typing.Dict[str, list], max_input_length=512, max_target_length=512
) -> typing.Dict[str, list]:
    """Converts data to tokenized data.

    Args:
        data: Data values which contain the keys input_text, prefix
          and target_text.
        max_input_length: Max length of the input string.
            Maximum length of (`int(1e30)`). Defaults to 512.
        max_target_length: Max length of the output string.
          Maximum length of (`int(1e30)`). Defaults to 512.

    Returns:
        Tokenizes the data (parameter).
    """
    inputs = [inp for inp, pre in zip(data["input_text"])]

    # model_inputs = tokenizer(
    #     inputs, max_length=max_input_length, truncation=True
    # )

    model_inputs = tokenizer.batch_encode_plus(
        inputs,
        max_length=max_input_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )

    # labels = tokenizer(
    #     text_target=data["target_text"],
    #     max_length=max_target_length,
    #     truncation=True,
    # )

    labels = tokenizer.batch_encode_plus(
        data["target_text"],
        max_length=max_target_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )

    model_inputs["labels"] = labels["input_ids"]
    model_inputs.to(device)
    return model_inputs


def eval(
    val_data_path: typing.List[str],
    model: AutoModelForSeq2SeqLM,
    tokenizer: AutoTokenizer,
    args: Seq2SeqTrainingArguments,
    output_dir: str,
) -> typing.Dict[str, float]:
    """Evaluate the Seq2Seq model.

    Arg:
        val_data_path: The path to the evaluation dataset
        model: The model to evaluate
        tokenizer: The string tokenizer.
            Converts strings to tokenized strings.
        args: Testing Arguments

    Returns:
        The Rouge and Bleu scores of the model.
    """
    raw_dataset = load_datasets(val_data_path)
    tokenized_datasets = raw_dataset.map(preprocess_data, batched=True)

    trainer = Seq2SeqTrainer(
        model,
        args,
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )
    results = trainer.evaluate(tokenized_datasets["test"])
    preds = []
    for d in raw_dataset["test"]:
        pred = predict(d["input_text"], model, tokenizer)
        preds.append(
            {"pred": pred, "label": d["target_text"], "input": d["input_text"]}
        )
    all_res = [results, preds]

    with open(
        os.path.join(
            output_dir,
            str("_with_".join(val_data_path).split("/")[-1]) + ".json",
        ),
        "w",
    ) as file:
        json.dump(all_res, file, indent=4)

    logger.info(f"Metric results: {results}")
    return results


if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        help="location of a YAML config file",
        default="src/model_configs/config.yaml",
    )
    args = parser.parse_args()

    options = vars(args)
    config = read_config_file(options["config"])
    output_dir = config["output_dir"]
    metrics = config["metrics"]
    args = init_args(
        config["hyper parameters"],
        output_dir,
    )
    wandb_dataset_tags, dataset_name = get_wandb_tags(config)
    # wandb.init(
    #     tags=wandb_dataset_tags,
    #     project="question generation evaluation",
    # )
    # wandb.config.update(args.to_dict())

    if "model_checkpoint" in config:
        model_checkpoint = get_latest_checkpoint_path(config)
        model_path = model_checkpoint
    if "hf_model" in config:
        model_path = config["hf_model"]
    model_path = "Factiverse/T5-base-questgen"
    if "bloom" in config["model_checkpoint"]:
        tokenizer = BloomTokenizerFast.from_pretrained(
            model_path, use_auth_token=True
        )
        model = BloomForCausalLM.from_pretrained(model_path)
    else:
        tokenizer = T5TokenizerFast.from_pretrained(
            model_path, use_auth_token=True
        )
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    model.to(device)

    # logger.info(
    #     "type a claim to see how the model responds \
    #              (q+enter to quit)"
    # )

    eval(config["data"], model, tokenizer, args, model_path)
    print("Enter a claim:")
    while True:
        inp = input()
        if inp == "q":
            break
        input_ids = tokenizer(inp, return_tensors="pt").to(device)
        print(input_ids["input_ids"])
        # outputs = model.generate(input_ids["input_ids"], max_new_tokens=512,
        #     do_sample=False,
        #     top_k=50,
        #     top_p=0.95,
        #     num_return_sequences=3,
        #     repetition_penalty=1.2,
        #     temperature=1.5,
        #     epsilon_cutoff=3e-4,
        #     diversity_penalty=1.99,
        #     num_beam_groups=3,
        #     num_beams=9
        #     )
        outputs = model.generate(
            input_ids["input_ids"],
            num_beams=5,
            num_beam_groups=5,
            max_new_tokens=30,
            diversity_penalty=1.0,
            num_return_sequences=3,
        )
        for i in range(outputs.shape[0]):
            print(f"Output {i+1}:")
            output_text = tokenizer.decode(outputs[i], skip_special_tokens=True)
            print(output_text)
