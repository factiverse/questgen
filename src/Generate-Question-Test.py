"""Test and Evaluate Seq2Seq Language Model."""

from transformers import (  # type: ignore
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    EvalPrediction,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)
import yaml  # type: ignore
import argparse
import os
import logging
from pathlib import Path
import typing
import numpy as np
import nltk  # type: ignore
import datasets  # type: ignore
import evaluate  # type: ignore
import torch
import json
from Load_Data import load_data, load_datasets
from util import read_config_file, init_args

logging.basicConfig(
    format="%(asctime)s %(levelname)-4s [%(name)s:%(lineno)d] - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)
metrics: typing.Dict[str, bool] = {}


def get_latest_checkpoint_path(config: dict) -> str:
    """Get last checkpoint to test on most recently trained model.

    Args:
        config: config file for training model
    """
    model_zoo = list(os.listdir(config["output_dir"]))
    model_checkpoint_models = []
    for m in model_zoo:
        if m.startswith(config["model_checkpoint"].split("/")[-1]):
            model_checkpoint_models.append(m)
    model_checkpoint_models.sort()
    model = "models/" + model_checkpoint_models[0]
    checkpoints = os.listdir(model)
    latest_checkpoint = "checkpoint-0"

    for checkpoint in checkpoints:
        if checkpoint.startswith("checkpoint-"):
            if int(latest_checkpoint.split("-")[-1]) < int(
                checkpoint.split("-")[-1]
            ):
                latest_checkpoint = checkpoint

    model_checkpoint = os.path.join(model, latest_checkpoint)

    if os.path.exists(model_checkpoint):
        logger.info(f"Using directory {model_checkpoint} for testing")
    else:
        logger.error(
            FileNotFoundError(
                f"The directory {model_checkpoint}\
                    does not exist. There are no greater\
                    checkpoints past this checkpoint"
            ),
            exc_info=True,
        )
    return model_checkpoint


def predict(
    to_predict: str, model: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer
) -> str:
    """Generates a response from the loaded model.

    Args:
        to_predict: Fed into the model as input.
        model: The Seq2Seq model.
        tokenizer: Used to tokenize input for the Seq2Seq model.

    Returns:
        the response generated by the requested Seq2Seq model.
    """
    input_ids = tokenizer(to_predict, return_tensors="pt").to(device)
    outputs = model.generate(input_ids["input_ids"])
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def compute_metrics(eval_pred: EvalPrediction) -> typing.Dict[str, float]:
    """Computes a custom metric combining BLEU and ROUGE.

    Using both bleu and rouge scores, compute_metrics evaluates the
    model's predictions comparing it to the target. 4 kinds
    of rouge scores and 1 bleu score is returned.

    Args:
        eval_pred: Evaluation results from the test dataset.
        rouge: If rouge should be used as a metric.
        bleu: If bleu should be used as a metric.

    Returns:
        Rouge and Bleu scores if requested.
    """
    global metrics

    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt")

    rouge = metrics["rouge"]
    bleu = metrics["bleu"]

    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(
        predictions, skip_special_tokens=True
    )
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Rouge expects a newline after each sentence
    decoded_preds = [
        "\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds
    ]
    decoded_labels = [
        "\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels
    ]

    # Note that other metrics may not have a `use_aggregator` parameter
    # and thus will return a list, computing a metric for each sentence.
    prediction_lens = [
        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions
    ]
    bleu_rouge_score = {}

    if rouge:
        metric_rouge = evaluate.load("rouge")
        result_rouge = metric_rouge.compute(
            predictions=decoded_preds,
            references=decoded_labels,
            use_stemmer=True,
            use_aggregator=True,
        )

        result_rouge["gen_len"] = np.mean(prediction_lens)

        bleu_rouge_score["rouge1"] = result_rouge["rouge1"]
        bleu_rouge_score["rouge2"] = result_rouge["rouge2"]
        bleu_rouge_score["rougeL"] = result_rouge["rougeL"]
        bleu_rouge_score["rougeLsum"] = result_rouge["rougeLsum"]

    if bleu:
        metric_bleu = evaluate.load("bleu")
        result_bleu = metric_bleu.compute(
            predictions=decoded_preds, references=decoded_labels
        )
        result_bleu["gen_len"] = np.mean(prediction_lens)

        bleu_rouge_score["bleu"] = result_bleu["bleu"]

    # Add mean generated length
    return bleu_rouge_score


def preprocess_data(
    data: typing.Dict[str, list], max_input_length=512, max_target_length=512
) -> typing.Dict[str, list]:
    """Converts data to tokenized data.

    Args:
        data: Data values which contain the keys input_text, prefix
          and target_text.
        max_input_length: Max length of the input string.
            Maximum length of (`int(1e30)`). Defaults to 512.
        max_target_length: Max length of the output string.
          Maximum length of (`int(1e30)`). Defaults to 512.

    Returns:
        Tokenizes the data (parameter).
    """
    inputs = [
        pre + ": " + inp for inp, pre in zip(data["input_text"], data["prefix"])
    ]

    # model_inputs = tokenizer(
    #     inputs, max_length=max_input_length, truncation=True
    # )

    model_inputs = tokenizer.batch_encode_plus(
        inputs,
        max_length=max_input_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )

    # labels = tokenizer(
    #     text_target=data["target_text"],
    #     max_length=max_target_length,
    #     truncation=True,
    # )

    labels = tokenizer.batch_encode_plus(
        data["target_text"],
        max_length=max_target_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )

    model_inputs["labels"] = labels["input_ids"]
    model_inputs.to(device)
    return model_inputs


def top_k_top_p_filtering(logits, top_k=50, top_p=0.95):
    # Sort logits and keep top k tokens only
    top_k = min(top_k, logits.size(-1))
    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
    logits[indices_to_remove] = -float("Inf")

    # Convert logits to probabilities
    probabilities = torch.nn.functional.softmax(logits, dim=-1)

    # Sort the probabilities to identify the cumulative sum up to top_p
    sorted_probabilities, sorted_indices = torch.sort(
        probabilities, descending=True
    )
    cumulative_probabilities = torch.cumsum(sorted_probabilities, dim=-1)

    # Remove tokens with a cumulative probability above the threshold
    sorted_indices_to_remove = cumulative_probabilities > top_p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[
        ..., :-1
    ].clone()
    sorted_indices_to_remove[..., 0] = 0

    indices_to_remove = sorted_indices_to_remove.scatter(
        dim=-1, index=sorted_indices, src=sorted_indices_to_remove
    )
    probabilities[indices_to_remove] = 0

    return probabilities


def generate_sequences(
    input_text, model, tokenizer, num_sequences=3, max_length=50
):
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    generated_sequences = []

    for _ in range(num_sequences):
        output_sequence = input_ids
        for _ in range(max_length):
            outputs = model(output_sequence, return_dict=True)
            next_token_logits = outputs.logits[:, -1, :]
            filtered_probabilities = top_k_top_p_filtering(next_token_logits)
            next_token = torch.multinomial(filtered_probabilities, 1)
            output_sequence = torch.cat([output_sequence, next_token], dim=-1)

            # Stop generating if the model produces the end-of-sequence token
            if next_token.item() == tokenizer.eos_token_id:
                break

        decoded_sequence = tokenizer.decode(
            output_sequence[0], skip_special_tokens=True
        )
        generated_sequences.append(decoded_sequence)

    return generated_sequences


def eval(
    val_data_path: typing.List[str],
    model: AutoModelForSeq2SeqLM,
    tokenizer: AutoTokenizer,
    args: Seq2SeqTrainingArguments,
    output_dir: str,
) -> typing.Dict[str, float]:
    """Evaluate the Seq2Seq model.

    Arg:
        val_data_path: The path to the evaluation dataset
        model: The model to evaluate
        tokenizer: The string tokenizer.
            Converts strings to tokenized strings.
        args: Testing Arguments

    Returns:
        The Rouge and Bleu scores of the model.
    """
    raw_dataset = load_datasets(val_data_path)
    tokenized_datasets = raw_dataset.map(preprocess_data, batched=True)

    trainer = Seq2SeqTrainer(
        model,
        args,
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )
    results = trainer.evaluate(tokenized_datasets["test"])
    preds = []
    for d in raw_dataset["test"]:
        pred = predict(d["input_text"], model, tokenizer)
        preds.append(
            {"pred": pred, "label": d["target_text"], "input": d["input_text"]}
        )
    all_res = [results, preds]

    with open(
        os.path.join(
            output_dir,
            str("_with_".join(val_data_path).split("/")[-1]) + ".json",
        ),
        "w",
    ) as file:
        json.dump(all_res, file, indent=4)

    logger.info(f"Metric results: {results}")
    return results


if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        help="location of a YAML config file",
        default="src/model_configs/config.yaml",
    )
    args = parser.parse_args()

    options = vars(args)
    config = read_config_file(options["config"])
    output_dir = config["output_dir"]
    metrics = config["metrics"]
    args = init_args(
        config["hyper parameters"],
        output_dir,
    )
    if "model_checkpoint" in config:
        model_checkpoint = get_latest_checkpoint_path(config)
        model_path = model_checkpoint
    if "hf_model" in config:
        model_path = config["hf_model"]
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, use_auth_token=True, device=device
    )

    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    model.to(device)

    # logger.info(
    #     "type a claim to see how the model responds \
    #              (q+enter to quit)"
    # )

    eval(
        config["data"],
        model,
        tokenizer,
        args,
        model_path,
        output_dir=output_dir,
    )
    # while True:
    #     inp = input()
    #     if inp == "q":
    #         break
    #     input_ids = tokenizer(inp, return_tensors="pt")
    #     print(input_ids["input_ids"])
    #     outputs = model.generate(input_ids["input_ids"])
    #     print(tokenizer.decode(outputs[0], skip_special_tokens=True))
