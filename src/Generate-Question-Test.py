"""Test and Evaluate Seq2Seq Language Model."""

from transformers import (  # type: ignore
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    EvalPrediction,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    BloomForCausalLM,
    BloomTokenizerFast,
    T5TokenizerFast,
)
import argparse
import os
import logging
import typing
import numpy as np
import nltk  # type: ignore
import evaluate  # type: ignore
import torch
import json
from Load_Data import load_datasets  # type: ignore
from util import (  # type: ignore
    get_wandb_tags_test,
    read_config_file,
    init_args,
)

logging.basicConfig(
    format="%(asctime)s %(levelname)-4s [%(name)s:%(lineno)d] - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)
metrics: typing.Dict[str, bool] = {}


def get_latest_checkpoint_path(config: dict) -> str:
    """Get last checkpoint to test on most recently trained model.

    Args:
        config: config file for training model
    """
    model_zoo = list(os.listdir(config["output_dir"]))
    model_checkpoint_models = []
    for m in model_zoo:
        if m.startswith(config["model_checkpoint"].split("/")[-1]):
            model_checkpoint_models.append(m)
    model_checkpoint_models.sort()
    model = "models/" + model_checkpoint_models[0]
    checkpoints = os.listdir(model)
    latest_checkpoint = "checkpoint-0"

    for checkpoint in checkpoints:
        if checkpoint.startswith("checkpoint-"):
            if int(latest_checkpoint.split("-")[-1]) < int(
                checkpoint.split("-")[-1]
            ):
                latest_checkpoint = checkpoint

    model_checkpoint = os.path.join(model, latest_checkpoint)

    if os.path.exists(model_checkpoint):
        logger.info(f"Using directory {model_checkpoint} for testing")
    else:
        logger.error(
            FileNotFoundError(
                f"The directory {model_checkpoint}\
                    does not exist. There are no greater\
                    checkpoints past this checkpoint"
            ),
            exc_info=True,
        )
    return model_checkpoint


def predict(
    to_predict: str,
    model: AutoModelForSeq2SeqLM,
    tokenizer: AutoTokenizer,
    preds: int = 1,
) -> list[str]:
    """Generates a response from the loaded model.

    Args:
        to_predict: Fed into the model as input.
        model: The Seq2Seq model.
        tokenizer: Used to tokenize input for the Seq2Seq model.
        preds: The number of predictions to generate.

    Returns:
        the response generated by the requested Seq2Seq model.
    """
    input_ids = tokenizer(to_predict, return_tensors="pt").to(device)
    res = []
    outputs = model.generate(
        input_ids["input_ids"],
        max_new_tokens=512,
        do_sample=False,
        top_k=50,
        top_p=0.95,
        num_return_sequences=preds,
        repetition_penalty=1.2,
        temperature=1.5,
        epsilon_cutoff=3e-4,
        diversity_penalty=0.5,
        num_beam_groups=3,
        num_beams=9,
    )
    for sample_output in outputs:
        res.append(tokenizer.decode(sample_output, skip_special_tokens=True))
    return res


def compute_metric(
    predictions: typing.List[str], labels: typing.List[str]
) -> typing.Dict[str, float]:
    """Computes a custom metric combining BLEU and ROUGE.

    This function is similar to the overloaded version,
    but takes in a list of strings of predictions and
    labels instead of EvalPredictions.

    Args:
        predictions: List of predictions generated by the model
        labels: List of target strings

    Returns:
        Rouge and Bleu scores if requested.
    """
    global metrics

    rouge = metrics["rouge"]
    bleu = metrics["bleu"]
    bleu_rouge_score = {}

    if rouge:
        metric_rouge = evaluate.load("rouge")
        result_rouge = metric_rouge.compute(
            predictions=predictions,
            references=labels,
            use_stemmer=True,
            use_aggregator=True,
        )

        bleu_rouge_score["rouge1"] = result_rouge["rouge1"]
        bleu_rouge_score["rouge2"] = result_rouge["rouge2"]
        bleu_rouge_score["rougeL"] = result_rouge["rougeL"]
        bleu_rouge_score["rougeLsum"] = result_rouge["rougeLsum"]

    if bleu:
        metric_bleu = evaluate.load("bleu")
        result_bleu = metric_bleu.compute(
            predictions=predictions, references=labels
        )

        bleu_rouge_score["bleu"] = result_bleu["bleu"]

    # Add mean generated length
    return bleu_rouge_score


def compute_metrics(eval_pred: EvalPrediction) -> typing.Dict[str, float]:
    """Computes a custom metric combining BLEU and ROUGE.

    Using both bleu and rouge scores, compute_metrics evaluates the
    model's predictions comparing it to the target. 4 kinds
    of rouge scores and 1 bleu score is returned.

    Args:
        eval_pred: Evaluation results from the test dataset.
        rouge: If rouge should be used as a metric.
        bleu: If bleu should be used as a metric.

    Returns:
        Rouge and Bleu scores if requested.
    """
    global metrics

    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt")

    rouge = metrics["rouge"]
    bleu = metrics["bleu"]

    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(
        predictions, skip_special_tokens=True
    )
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Rouge expects a newline after each sentence
    decoded_preds = [
        "\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds
    ]
    decoded_labels = [
        "\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels
    ]

    # Note that other metrics may not have a `use_aggregator` parameter
    # and thus will return a list, computing a metric for each sentence.
    prediction_lens = [
        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions
    ]
    bleu_rouge_score = {}

    if rouge:
        metric_rouge = evaluate.load("rouge")
        result_rouge = metric_rouge.compute(
            predictions=decoded_preds,
            references=decoded_labels,
            use_stemmer=True,
            use_aggregator=True,
        )

        result_rouge["gen_len"] = np.mean(prediction_lens)

        bleu_rouge_score["rouge1"] = result_rouge["rouge1"]
        bleu_rouge_score["rouge2"] = result_rouge["rouge2"]
        bleu_rouge_score["rougeL"] = result_rouge["rougeL"]
        bleu_rouge_score["rougeLsum"] = result_rouge["rougeLsum"]

    if bleu:
        metric_bleu = evaluate.load("bleu")
        result_bleu = metric_bleu.compute(
            predictions=decoded_preds, references=decoded_labels
        )
        result_bleu["gen_len"] = np.mean(prediction_lens)

        bleu_rouge_score["bleu"] = result_bleu["bleu"]

    # Add mean generated length
    return bleu_rouge_score


def preprocess_data(
    data: typing.Dict[str, list], max_input_length=512, max_target_length=512
) -> typing.Dict[str, list]:
    """Converts data to tokenized data.

    Args:
        data: Data values which contain the keys input_text, prefix
          and target_text.
        max_input_length: Max length of the input string.
            Maximum length of (`int(1e30)`). Defaults to 512.
        max_target_length: Max length of the output string.
          Maximum length of (`int(1e30)`). Defaults to 512.

    Returns:
        Tokenizes the data (parameter).
    """
    inputs = [
        pre + ": " + inp for inp, pre in zip(data["input_text"], data["prefix"])
    ]

    # model_inputs = tokenizer(
    #     inputs, max_length=max_input_length, truncation=True
    # )

    model_inputs = tokenizer.batch_encode_plus(
        inputs,
        max_length=max_input_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )

    # labels = tokenizer(
    #     text_target=data["target_text"],
    #     max_length=max_target_length,
    #     truncation=True,
    # )

    labels = tokenizer.batch_encode_plus(
        data["target_text"],
        max_length=max_target_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )

    model_inputs["labels"] = labels["input_ids"]
    model_inputs.to(device)
    return model_inputs


def eval(
    val_data_path: typing.List[str],
    model: AutoModelForSeq2SeqLM,
    tokenizer: AutoTokenizer,
    args: Seq2SeqTrainingArguments,
    output_dir: str,
) -> typing.Dict[str, float]:
    """Evaluate the Seq2Seq model.

    Arg:
        val_data_path: The path to the evaluation dataset
        model: The model to evaluate
        tokenizer: The string tokenizer.
            Converts strings to tokenized strings.
        args: Testing Arguments

    Returns:
        The Rouge and Bleu scores of the model.
    """
    if isinstance(val_data_path, str):
        val_data_path = [val_data_path]

    raw_dataset = load_datasets(val_data_path)
    tokenized_datasets = raw_dataset.map(preprocess_data, batched=True)

    trainer = Seq2SeqTrainer(
        model,
        args,
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )
    results = trainer.evaluate(tokenized_datasets["test"])
    preds = []
    freq: typing.Dict[str, typing.List[str]] = {}
    for example in raw_dataset["test"]:
        if example["input_text"] in freq:
            freq[example["input_text"]].append(example["target_text"])
        else:
            freq[example["input_text"]] = [example["target_text"]]

    for inp, target in freq.items():
        pred = predict(inp, model, tokenizer, min(len(target), 3))
        pred = [p.strip() for p in pred]
        empty_list = all(not element for element in pred)
        if len(target) > 3:
            target = target[:3]
        if empty_list:
            metrics = {
                "rouge1": 0.0,
                "rouge2": 0.0,
                "rougeL": 0.0,
                "rougeLsum": 0.0,
                "bleu": 0.0,
            }
        else:
            metrics = compute_metric(pred, target)
        preds.append(
            {"pred": pred, "label": target, "input": inp, "metrics": metrics}
        )


    # for inp in raw_dataset["test"]:
    #     pred = predict(inp['input_text'], model, tokenizer, 3)
    #     pred = [p.strip() for p in pred]       
    #     preds.append(
    #         {"pred": pred, "input": inp}
    #     )
    all_res = [preds,
               results]

    print(output_dir)

    with open(
        os.path.join(
            output_dir,
            str("_with_".join([d.split("/")[-1] for d in val_data_path]))
            + ".json",
        ),
        "w",
    ) as file:
        json.dump(all_res, file, indent=4)

    # logger.info(f"Metric results: {results}")
    # return results


if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config",
        help="location of a YAML config file",
        default="src/model_configs/config.yaml",
    )
    args = parser.parse_args()

    options = vars(args)
    config = read_config_file(options["config"])
    output_dir = config["output_dir"]
    metrics = config["metrics"]
    args = init_args(
        config["hyper parameters"],
        output_dir,
    )
    wandb_dataset_tags, dataset_name = get_wandb_tags_test(config)
    # wandb.init(
    #     tags=wandb_dataset_tags,
    #     project="question generation evaluation",
    # )
    # wandb.config.update(args.to_dict())

    if "model_checkpoint" in config:
        model_checkpoint = get_latest_checkpoint_path(config)
        model_path = model_checkpoint
    if "hf_model" in config:
        model_path = config["hf_model"]

    if "bloom" in config["model_checkpoint"]:
        tokenizer = BloomTokenizerFast.from_pretrained(
            model_path, use_auth_token=True
        )
        model = BloomForCausalLM.from_pretrained(model_path)
    else:
        tokenizer = T5TokenizerFast.from_pretrained(
            model_path, use_auth_token=True
        )
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    model.to(device)

    eval(config["test_data"], model, tokenizer, args, model_path)

    # logger.info(
    #     "type a claim to see how the model responds \
    #              (q+enter to quit)"
    # )
    # while True:
    #     inp = input()
    #     if inp == "q":
    #         break
    #     input_ids = tokenizer(inp, return_tensors="pt").to(device)
    #     print(input_ids["input_ids"])
    #     # outputs = model.generate(input_ids["input_ids"], max_new_tokens=512,
    #     #     do_sample=False,
    #     #     top_k=50,
    #     #     top_p=0.95,
    #     #     num_return_sequences=3,
    #     #     repetition_penalty=1.2,
    #     #     temperature=1.5,
    #     #     epsilon_cutoff=3e-4,
    #     #     diversity_penalty=1.99,
    #     #     num_beam_groups=3,
    #     #     num_beams=9
    #     #     )
    #     outputs = model.generate(
    #         input_ids["input_ids"],
    #         num_beams=5,
    #         num_beam_groups=5,
    #         max_new_tokens=30,
    #         diversity_penalty=1.0,
    #         num_return_sequences=3,
    #     )
    #     for i in range(outputs.shape[0]):
    #         print(f"Output {i+1}:")
    #  output_text = tokenizer.decode(outputs[i],skip_special_tokens=True)
    #         print(output_text)
