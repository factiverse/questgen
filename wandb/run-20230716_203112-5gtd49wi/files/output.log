Found cached dataset json (/home/ritvik/.cache/huggingface/datasets/json/default-73090ac240390b78/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 246.68it/s]
Found cached dataset json (/home/ritvik/.cache/huggingface/datasets/json/default-be5be90bb1eb255e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 526.72it/s]
Loading cached processed dataset at /home/ritvik/.cache/huggingface/datasets/json/default-73090ac240390b78/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fba9b200414a86cf.arrow

Map:  65%|███████████████████████████████████████████████████████████████████▍                                   | 25000/38179 [00:01<00:00, 21292.35 examples/s]
/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                                                                                  | 0/23870 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|                                                                                                                       | 1/23870 [00:10<70:10:05, 10.58s/it]Traceback (most recent call last):
  File "/home/ritvik/questgen/src/QuestionGeneration/T5/Generate-Question-train.py", line 146, in <module>
    trainer.train()
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/transformers/trainer.py", line 2759, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/transformers/trainer.py", line 2784, in compute_loss
    outputs = model(**inputs)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 172, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 184, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 86, in gather
    res = gather_map(outputs)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 77, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 12, in __init__
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/transformers/utils/generic.py", line 277, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 77, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 81, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 81, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 71, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/ritvik/conda/envs/testing-reqs/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 235, in gather
    return torch._C._gather(tensors, dim, destination)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 14.62 GiB total capacity; 13.63 GiB already allocated; 5.38 MiB free; 13.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF