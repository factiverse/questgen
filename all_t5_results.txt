src/model_configs/config-t5-base-all_claim_decomp.yaml
2023-11-17 08:43:22 INFO [__main__:72] - Using directory models/t5-base_all_0/checkpoint-41672 for testing
2023-11-17 08:43:27 INFO [__main__:370] - type a claim to see how the model responds                  (q+enter to quit)
2023-11-17 08:43:27 WARNING [datasets.builder:816] - Found cached dataset json (/home/ritvik/.cache/huggingface/datasets/json/default-990326b0491caaa8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 775.57it/s]
2023-11-17 08:43:27 WARNING [datasets.arrow_dataset:3062] - Loading cached processed dataset at /home/ritvik/.cache/huggingface/datasets/json/default-990326b0491caaa8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-26e5245dc51a3cce.arrow
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/ritvik/conda/envs/simpletransformers/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.04s/it]2023-11-17 08:43:46 INFO [absl:83] - Using default tokenizer.
wandb: Currently logged in as: rawpower9 (factiverse-ai). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ritvik/questgen/wandb/run-20231117_084348-rmdqdvnc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-night-85
wandb: â­ï¸ View project at https://wandb.ai/factiverse-ai/huggingface
wandb: ğŸš€ View run at https://wandb.ai/factiverse-ai/huggingface/runs/rmdqdvnc
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.28s/it]
/home/ritvik/conda/envs/simpletransformers/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
2023-11-17 08:44:15 INFO [__main__:332] - Metric results: {'eval_loss': 23.838085174560547, 'eval_rouge1': 0.8112380718646072, 'eval_rouge2': 0.7110073523834368, 'eval_rougeL': 0.7845829920573408, 'eval_rougeLsum': 0.7843397088720162, 'eval_bleu': 0.6332500448762127, 'eval_runtime': 19.2097, 'eval_samples_per_second': 5.622, 'eval_steps_per_second': 0.104}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:               eval/bleu â–
wandb:               eval/loss â–
wandb:             eval/rouge1 â–
wandb:             eval/rouge2 â–
wandb:             eval/rougeL â–
wandb:          eval/rougeLsum â–
wandb:            eval/runtime â–
wandb: eval/samples_per_second â–
wandb:   eval/steps_per_second â–
wandb:       train/global_step â–
wandb: 
wandb: Run summary:
wandb:               eval/bleu 0.63325
wandb:               eval/loss 23.83809
wandb:             eval/rouge1 0.81124
wandb:             eval/rouge2 0.71101
wandb:             eval/rougeL 0.78458
wandb:          eval/rougeLsum 0.78434
wandb:            eval/runtime 19.2097
wandb: eval/samples_per_second 5.622
wandb:   eval/steps_per_second 0.104
wandb:       train/global_step 0
wandb: 
wandb: ğŸš€ View run denim-night-85 at: https://wandb.ai/factiverse-ai/huggingface/runs/rmdqdvnc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231117_084348-rmdqdvnc/logs
src/model_configs/config-t5-base-all_fact_checking_briefs.yaml
2023-11-17 08:44:25 INFO [__main__:72] - Using directory models/t5-base_all_0/checkpoint-41672 for testing
2023-11-17 08:44:30 INFO [__main__:370] - type a claim to see how the model responds                  (q+enter to quit)
2023-11-17 08:44:30 WARNING [datasets.builder:816] - Found cached dataset json (/home/ritvik/.cache/huggingface/datasets/json/default-7550d3928950918d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 775.57it/s]
Map:   0%|          | 0/145 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145/145 [00:00<00:00, 1288.17 examples/s]                                                               You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/ritvik/conda/envs/simpletransformers/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/3 [00:00<?, ?it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.44s/it]2023-11-17 08:44:52 INFO [absl:83] - Using default tokenizer.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: rawpower9 (factiverse-ai). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ritvik/questgen/wandb/run-20231117_084455-v2an3znu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-water-86
wandb: â­ï¸ View project at https://wandb.ai/factiverse-ai/huggingface
wandb: ğŸš€ View run at https://wandb.ai/factiverse-ai/huggingface/runs/v2an3znu
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.53s/it]
/home/ritvik/conda/envs/simpletransformers/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
2023-11-17 08:45:30 INFO [__main__:332] - Metric results: {'eval_loss': 22.01308250427246, 'eval_rouge1': 0.3224454721490616, 'eval_rouge2': 0.11660065045796912, 'eval_rougeL': 0.2899706940329796, 'eval_rougeLsum': 0.2895069925903709, 'eval_bleu': 0.07602692434987163, 'eval_runtime': 22.7671, 'eval_samples_per_second': 6.369, 'eval_steps_per_second': 0.132}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)wandb: \ 0.023 MB of 0.023 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:               eval/bleu â–
wandb:               eval/loss â–
wandb:             eval/rouge1 â–
wandb:             eval/rouge2 â–
wandb:             eval/rougeL â–
wandb:          eval/rougeLsum â–
wandb:            eval/runtime â–
wandb: eval/samples_per_second â–
wandb:   eval/steps_per_second â–
wandb:       train/global_step â–
wandb: 
wandb: Run summary:
wandb:               eval/bleu 0.07603
wandb:               eval/loss 22.01308
wandb:             eval/rouge1 0.32245
wandb:             eval/rouge2 0.1166
wandb:             eval/rougeL 0.28997
wandb:          eval/rougeLsum 0.28951
wandb:            eval/runtime 22.7671
wandb: eval/samples_per_second 6.369
wandb:   eval/steps_per_second 0.132
wandb:       train/global_step 0
wandb: 
wandb: ğŸš€ View run confused-water-86 at: https://wandb.ai/factiverse-ai/huggingface/runs/v2an3znu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231117_084455-v2an3znu/logs
src/model_configs/config-t5-base-all_gpt_generated.yaml
2023-11-17 08:45:40 INFO [__main__:72] - Using directory models/t5-base_all_0/checkpoint-41672 for testing
2023-11-17 08:45:45 INFO [__main__:370] - type a claim to see how the model responds                  (q+enter to quit)
2023-11-17 08:45:45 WARNING [datasets.builder:816] - Found cached dataset json (/home/ritvik/.cache/huggingface/datasets/json/default-827ab733db3a05b1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 685.23it/s]
Map:   0%|          | 0/3817 [00:00<?, ? examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 1000/3817 [00:00<00:01, 1440.43 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2000/3817 [00:01<00:01, 1767.31 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3000/3817 [00:01<00:00, 1920.31 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3817/3817 [00:02<00:00, 1955.23 examples/s]                                                                 You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/ritvik/conda/envs/simpletransformers/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/60 [00:00<?, ?it/s]  3%|â–         | 2/60 [00:05<02:51,  2.96s/it]  5%|â–Œ         | 3/60 [00:11<03:59,  4.21s/it]  7%|â–‹         | 4/60 [00:17<04:33,  4.88s/it]  8%|â–Š         | 5/60 [00:23<04:51,  5.30s/it] 10%|â–ˆ         | 6/60 [00:30<05:01,  5.58s/it] 12%|â–ˆâ–        | 7/60 [00:36<05:07,  5.80s/it] 13%|â–ˆâ–        | 8/60 [00:42<05:10,  5.97s/it] 15%|â–ˆâ–Œ        | 9/60 [00:49<05:11,  6.12s/it] 17%|â–ˆâ–‹        | 10/60 [00:55<05:12,  6.26s/it] 18%|â–ˆâ–Š        | 11/60 [01:02<05:09,  6.32s/it] 20%|â–ˆâ–ˆ        | 12/60 [01:08<05:03,  6.33s/it] 22%|â–ˆâ–ˆâ–       | 13/60 [01:14<04:56,  6.31s/it] 23%|â–ˆâ–ˆâ–       | 14/60 [01:21<04:49,  6.29s/it] 25%|â–ˆâ–ˆâ–Œ       | 15/60 [01:27<04:41,  6.25s/it] 27%|â–ˆâ–ˆâ–‹       | 16/60 [01:33<04:33,  6.22s/it] 28%|â–ˆâ–ˆâ–Š       | 17/60 [01:39<04:27,  6.21s/it] 30%|â–ˆâ–ˆâ–ˆ       | 18/60 [01:45<04:20,  6.21s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 19/60 [01:51<04:14,  6.21s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 20/60 [01:58<04:09,  6.25s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/60 [02:04<04:04,  6.27s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 22/60 [02:10<03:58,  6.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 23/60 [02:17<03:52,  6.29s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/60 [02:23<03:46,  6.30s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 25/60 [02:29<03:40,  6.30s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/60 [02:36<03:33,  6.29s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/60 [02:42<03:27,  6.28s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 28/60 [02:48<03:20,  6.27s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 29/60 [02:54<03:13,  6.26s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/60 [03:01<03:08,  6.27s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 31/60 [03:07<03:01,  6.26s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 32/60 [03:13<02:55,  6.25s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/60 [03:19<02:48,  6.25s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 34/60 [03:26<02:42,  6.26s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 35/60 [03:32<02:36,  6.26s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/60 [03:38<02:30,  6.26s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 37/60 [03:44<02:24,  6.27s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 38/60 [03:51<02:17,  6.27s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/60 [03:57<02:11,  6.28s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 40/60 [04:03<02:05,  6.28s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 41/60 [04:10<01:59,  6.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/60 [04:16<01:53,  6.31s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 43/60 [04:22<01:47,  6.30s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 44/60 [04:29<01:40,  6.29s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 45/60 [04:35<01:34,  6.29s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 46/60 [04:41<01:27,  6.28s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 47/60 [04:47<01:21,  6.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 48/60 [04:54<01:15,  6.26s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 49/60 [05:00<01:08,  6.26s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 50/60 [05:06<01:02,  6.28s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 51/60 [05:12<00:56,  6.27s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 52/60 [05:19<00:50,  6.26s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 53/60 [05:25<00:43,  6.26s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 54/60 [05:31<00:37,  6.26s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 55/60 [05:37<00:31,  6.26s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 56/60 [05:44<00:25,  6.26s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 57/60 [05:50<00:18,  6.26s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 58/60 [05:56<00:12,  6.26s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 59/60 [06:03<00:06,  6.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [06:07<00:00,  5.68s/it]2023-11-17 08:52:09 INFO [absl:83] - Using default tokenizer.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: rawpower9 (factiverse-ai). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in /home/ritvik/questgen/wandb/run-20231117_085215-f76mz0hp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-dust-87
wandb: â­ï¸ View project at https://wandb.ai/factiverse-ai/huggingface
wandb: ğŸš€ View run at https://wandb.ai/factiverse-ai/huggingface/runs/f76mz0hp
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [06:19<00:00,  6.33s/it]
/home/ritvik/conda/envs/simpletransformers/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
2023-11-17 09:04:56 INFO [__main__:332] - Metric results: {'eval_loss': 21.598352432250977, 'eval_rouge1': 0.4384017038045116, 'eval_rouge2': 0.22233067829248093, 'eval_rougeL': 0.39414469811285324, 'eval_rougeLsum': 0.39445789163947204, 'eval_bleu': 0.12043661030354871, 'eval_runtime': 386.5538, 'eval_samples_per_second': 9.874, 'eval_steps_per_second': 0.155}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               eval/bleu â–
wandb:               eval/loss â–
wandb:             eval/rouge1 â–
wandb:             eval/rouge2 â–
wandb:             eval/rougeL â–
wandb:          eval/rougeLsum â–
wandb:            eval/runtime â–
wandb: eval/samples_per_second â–
wandb:   eval/steps_per_second â–
wandb:       train/global_step â–
wandb: 
wandb: Run summary:
wandb:               eval/bleu 0.12044
wandb:               eval/loss 21.59835
wandb:             eval/rouge1 0.4384
wandb:             eval/rouge2 0.22233
wandb:             eval/rougeL 0.39414
wandb:          eval/rougeLsum 0.39446
wandb:            eval/runtime 386.5538
wandb: eval/samples_per_second 9.874
wandb:   eval/steps_per_second 0.155
wandb:       train/global_step 0
wandb: 
wandb: ğŸš€ View run amber-dust-87 at: https://wandb.ai/factiverse-ai/huggingface/runs/f76mz0hp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231117_085215-f76mz0hp/logs
src/model_configs/config-t5-base-all_faviq_r_set.yaml
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /home/ritvik/conda/envs/simpletransformers/lib/python3.9/runpy.py:197 in _run_module_as_main     â”‚
â”‚                                                                                                  â”‚
â”‚   194 â”‚   main_globals = sys.modules["__main__"].__dict__                                        â”‚
â”‚   195 â”‚   if alter_argv:                                                                         â”‚
â”‚   196 â”‚   â”‚   sys.argv[0] = mod_spec.origin                                                      â”‚
â”‚ â± 197 â”‚   return _run_code(code, main_globals, None,                                             â”‚
â”‚   198 â”‚   â”‚   â”‚   â”‚   â”‚    "__main__", mod_spec)                                                 â”‚
â”‚   199                                                                                            â”‚
â”‚   200 def run_module(mod_name, init_globals=None,                                                â”‚
â”‚                                                                                                  â”‚
â”‚ /home/ritvik/conda/envs/simpletransformers/lib/python3.9/runpy.py:87 in _run_code                â”‚
â”‚                                                                                                  â”‚
â”‚    84 â”‚   â”‚   â”‚   â”‚   â”‚      __loader__ = loader,                                                â”‚
â”‚    85 â”‚   â”‚   â”‚   â”‚   â”‚      __package__ = pkg_name,                                             â”‚
â”‚    86 â”‚   â”‚   â”‚   â”‚   â”‚      __spec__ = mod_spec)                                                â”‚
â”‚ â±  87 â”‚   exec(code, run_globals)                                                                â”‚
â”‚    88 â”‚   return run_globals                                                                     â”‚
â”‚    89                                                                                            â”‚
â”‚    90 def _run_module_code(code, init_globals=None,                                              â”‚
â”‚                                                                                                  â”‚
â”‚ /home/ritvik/questgen/src/Generate-Question-test.py:353 in <module>                              â”‚
â”‚                                                                                                  â”‚
â”‚   350 â”‚                                                                                          â”‚
â”‚   351 â”‚   options = vars(args)                                                                   â”‚
â”‚   352 â”‚                                                                                          â”‚
â”‚ â± 353 â”‚   config = read_config_file(options["config"])                                           â”‚
â”‚   354 â”‚   metrics = config["metrics"]                                                            â”‚
â”‚   355 â”‚                                                                                          â”‚
â”‚   356 â”‚   model_checkpoint = get_model_checkpoint_path(config)                                   â”‚
â”‚                                                                                                  â”‚
â”‚ /home/ritvik/questgen/src/Generate-Question-test.py:41 in read_config_file                       â”‚
â”‚                                                                                                  â”‚
â”‚    38 â”‚   Returns:                                                                               â”‚
â”‚    39 â”‚   â”‚   the contents of the YAML file                                                      â”‚
â”‚    40 â”‚   """                                                                                    â”‚
â”‚ â±  41 â”‚   with open(file_name, "r") as file:                                                     â”‚
â”‚    42 â”‚   â”‚   config = yaml.safe_load(file)                                                      â”‚
â”‚    43 â”‚   return config                                                                          â”‚
â”‚    44                                                                                            â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
FileNotFoundError: [Errno 2] No such file or directory: 'src/model_configs/config-t5-base-all_faviq_r_set.yaml'
src/model_configs/config-t5-base-all_faviq_a_set.yaml
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /home/ritvik/conda/envs/simpletransformers/lib/python3.9/runpy.py:197 in _run_module_as_main     â”‚
â”‚                                                                                                  â”‚
â”‚   194 â”‚   main_globals = sys.modules["__main__"].__dict__                                        â”‚
â”‚   195 â”‚   if alter_argv:                                                                         â”‚
â”‚   196 â”‚   â”‚   sys.argv[0] = mod_spec.origin                                                      â”‚
â”‚ â± 197 â”‚   return _run_code(code, main_globals, None,                                             â”‚
â”‚   198 â”‚   â”‚   â”‚   â”‚   â”‚    "__main__", mod_spec)                                                 â”‚
â”‚   199                                                                                            â”‚
â”‚   200 def run_module(mod_name, init_globals=None,                                                â”‚
â”‚                                                                                                  â”‚
â”‚ /home/ritvik/conda/envs/simpletransformers/lib/python3.9/runpy.py:87 in _run_code                â”‚
â”‚                                                                                                  â”‚
â”‚    84 â”‚   â”‚   â”‚   â”‚   â”‚      __loader__ = loader,                                                â”‚
â”‚    85 â”‚   â”‚   â”‚   â”‚   â”‚      __package__ = pkg_name,                                             â”‚
â”‚    86 â”‚   â”‚   â”‚   â”‚   â”‚      __spec__ = mod_spec)                                                â”‚
â”‚ â±  87 â”‚   exec(code, run_globals)                                                                â”‚
â”‚    88 â”‚   return run_globals                                                                     â”‚
â”‚    89                                                                                            â”‚
â”‚    90 def _run_module_code(code, init_globals=None,                                              â”‚
â”‚                                                                                                  â”‚
â”‚ /home/ritvik/questgen/src/Generate-Question-test.py:353 in <module>                              â”‚
â”‚                                                                                                  â”‚
â”‚   350 â”‚                                                                                          â”‚
â”‚   351 â”‚   options = vars(args)                                                                   â”‚
â”‚   352 â”‚                                                                                          â”‚
â”‚ â± 353 â”‚   config = read_config_file(options["config"])                                           â”‚
â”‚   354 â”‚   metrics = config["metrics"]                                                            â”‚
â”‚   355 â”‚                                                                                          â”‚
â”‚   356 â”‚   model_checkpoint = get_model_checkpoint_path(config)                                   â”‚
â”‚                                                                                                  â”‚
â”‚ /home/ritvik/questgen/src/Generate-Question-test.py:41 in read_config_file                       â”‚
â”‚                                                                                                  â”‚
â”‚    38 â”‚   Returns:                                                                               â”‚
â”‚    39 â”‚   â”‚   the contents of the YAML file                                                      â”‚
â”‚    40 â”‚   """                                                                                    â”‚
â”‚ â±  41 â”‚   with open(file_name, "r") as file:                                                     â”‚
â”‚    42 â”‚   â”‚   config = yaml.safe_load(file)                                                      â”‚
â”‚    43 â”‚   return config                                                                          â”‚
â”‚    44                                                                                            â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
FileNotFoundError: [Errno 2] No such file or directory: 'src/model_configs/config-t5-base-all_faviq_a_set.yaml'
